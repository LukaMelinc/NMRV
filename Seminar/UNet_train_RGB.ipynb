{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision albumentations segmentation_models_pytorch PIL torchmetrics torchmetrics tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = self._conv_block(3, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = self._conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = self._conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = self._conv_block(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.conv5 = self._conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv6 = self._conv_block(1024, 512)\n",
    "        self.up7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv7 = self._conv_block(512, 256)\n",
    "        self.up8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv8 = self._conv_block(256, 128)\n",
    "        self.up9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv9 = self._conv_block(128, 64)\n",
    "        self.conv10 = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "        c5 = self.conv5(p4)\n",
    "\n",
    "        # Decoder\n",
    "        up_6 = self.up6(c5)\n",
    "        merge6 = torch.cat([up_6, c4], dim=1)\n",
    "        c6 = self.conv6(merge6)\n",
    "        up_7 = self.up7(c6)\n",
    "        merge7 = torch.cat([up_7, c3], dim=1)\n",
    "        c7 = self.conv7(merge7)\n",
    "        up_8 = self.up8(c7)\n",
    "        merge8 = torch.cat([up_8, c2], dim=1)\n",
    "        c8 = self.conv8(merge8)\n",
    "        up_9 = self.up9(c8)\n",
    "        merge9 = torch.cat([up_9, c1], dim=1)\n",
    "        c9 = self.conv9(merge9)\n",
    "        c10 = self.conv10(c9)\n",
    "\n",
    "        return c10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def show_augmented_images(dataloader, num_samples=3):\n",
    "    # Define a color map for the classes\n",
    "    class_colors = {\n",
    "        0: [0, 0, 0],      # Background or Ignore\n",
    "        1: [0, 255, 0],    # Static Obstacle (Green)\n",
    "        2: [255, 0, 0],    # Dynamic Obstacle (Red)\n",
    "        3: [0, 0, 255]     # Water (Blue)\n",
    "        # Add more classes as needed\n",
    "    }\n",
    "\n",
    "    batch = next(iter(dataloader))  # Get one batch\n",
    "    images, masks = batch\n",
    "\n",
    "    images = images.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        img = np.transpose(images[i], (1, 2, 0))  # Convert from [C, H, W] to [H, W, C]\n",
    "\n",
    "        if img.max() <= 1.0:\n",
    "            img = (img * 255).astype(np.uint8)  # Convert back to 0-255 for display\n",
    "\n",
    "        mask = masks[i].squeeze()\n",
    "\n",
    "        # Debugging: Print unique values in the mask\n",
    "        #print(f\"Unique values in mask {i}: {np.unique(mask)}\")\n",
    "\n",
    "        # Create an RGB image for the mask\n",
    "        mask_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "        for class_label, color in class_colors.items():\n",
    "            mask_rgb[mask == class_label] = color\n",
    "\n",
    "        axes[i, 0].imshow(img)  # Show image\n",
    "        axes[i, 0].set_title(\"Augmented Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(mask_rgb)  # Show mask in RGB\n",
    "        axes[i, 1].set_title(\"Augmented Mask\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#from your_module import UNet, SegmentationDataset  # Make sure to import your UNet model and SegmentationDataset\n",
    "# Define the SegmentationDataset class\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_filenames[idx])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Convert mask to numpy array\n",
    "        mask_np = np.array(mask)\n",
    "        #print(f\"Unique values in mask {image}: {np.unique(mask)}\")\n",
    "\n",
    "        #mask = np.clip(mask, 0, 3)\n",
    "        if mask_np.max() <= 1.0:\n",
    "            mask_np = (mask_np * 255).astype(np.uint8)\n",
    "\n",
    "        # Adapt the mask to ignore classes 6 and 7, and consider class 5 as part of class 3\n",
    "        mask_np[mask_np == 5] = 3\n",
    "        mask_np[mask_np == 6] = 0\n",
    "        mask_np[mask_np == 7] = 0\n",
    "\n",
    "        # Debugging: Print unique values in the mask before adaptation\n",
    "        #print(f\"Unique values in mask before adaptation: {np.unique(mask_np)}\")\n",
    "\n",
    "        # Convert mask to tensor\n",
    "        mask = torch.from_numpy(mask_np).long()\n",
    "\n",
    "        # Debugging: Print unique values in the mask after adaptation\n",
    "        #print(f\"Unique values in mask after adaptation: {torch.unique(mask)}\")\n",
    "\n",
    "        return image, mask\n",
    "# Define the transformations for the training and validation sets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Add other transformations if needed\n",
    "])\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_dataset = SegmentationDataset(\n",
    "    image_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/images/train',\n",
    "    mask_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/masks/train',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    image_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/images/val',\n",
    "    mask_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/masks/val',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3, shuffle=False, num_workers=1)\n",
    "\n",
    "# Initialize the UNet model\n",
    "model = UNet(num_classes=4)  # Adjust the number of classes as needed\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        #show_augmented_images(train_loader)\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        masks = masks.squeeze(1).long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            masks = masks.squeeze(1).long()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
    "\n",
    "print('Training complete')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
