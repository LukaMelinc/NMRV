{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision albumentations segmentation_models_pytorch PIL torchmetrics torchmetrics tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PIL tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def up_conv(in_channels, out_channels):\n",
    "            return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(3, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = up_conv(1024, 512)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "        self.upconv3 = up_conv(512, 256)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.upconv2 = up_conv(256, 128)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.upconv1 = up_conv(128, 64)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        # Output layer\n",
    "        self.conv_final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "\n",
    "        # Decoder\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.dec4(dec4)\n",
    "\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "\n",
    "        return self.conv_final(dec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def show_augmented_images(dataloader, num_samples=3):\n",
    "    batch = next(iter(dataloader))  # Get one batch\n",
    "    images, masks  = batch\n",
    "\n",
    "    images = images.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        img = np.transpose(images[i], (1, 2, 0))  # Convert from [C, H, W] to [H, W, C]\n",
    "\n",
    "        if img.max() <= 1.0:\n",
    "            img = (img * 255).astype(np.uint8)  # Convert back to 0-255 for display\n",
    "\n",
    "        mask = masks[i].squeeze()\n",
    "\n",
    "        axes[i, 0].imshow(img)  # Show image\n",
    "        axes[i, 0].set_title(\"Augmentirana slika\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(mask, cmap=\"gray\")  # Show mask\n",
    "        axes[i, 1].set_title(\"Augmentirana maska\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#from your_module import UNet, SegmentationDataset  # Make sure to import your UNet model and SegmentationDataset\n",
    "# Define the SegmentationDataset class\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_filenames[idx])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Convert mask to numpy array\n",
    "        mask_np = np.array(mask)\n",
    "\n",
    "        # Adapt the mask to ignore classes 6 and 7, and consider class 5 as part of class 3\n",
    "        mask_np[mask_np == 5] = 3\n",
    "        mask_np[mask_np == 6] = 0\n",
    "        mask_np[mask_np == 7] = 0\n",
    "\n",
    "        # Debugging: Print unique values in the mask before adaptation\n",
    "        print(f\"Unique values in mask before adaptation: {np.unique(mask_np)}\")\n",
    "\n",
    "        # Convert mask to tensor\n",
    "        mask = torch.from_numpy(mask_np).long()\n",
    "\n",
    "        # Debugging: Print unique values in the mask after adaptation\n",
    "        print(f\"Unique values in mask after adaptation: {torch.unique(mask)}\")\n",
    "\n",
    "        return image, mask\n",
    "# Define the transformations for the training and validation sets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Add other transformations if needed\n",
    "])\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_dataset = SegmentationDataset(\n",
    "    image_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/images/train',\n",
    "    mask_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/masks/train',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    image_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/images/val',\n",
    "    mask_dir='/content/drive/MyDrive/RGB_datasets_segmentation_V2/masks/val',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3, shuffle=False, num_workers=1)\n",
    "\n",
    "# Initialize the UNet model\n",
    "model = UNet(num_classes=4)  # Adjust the number of classes as needed\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        #show_augmented_images(train_loader)\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        masks = masks.squeeze(1).long()\n",
    "\n",
    "        #print(f\"unique values in masks: {torch.unique(masks)}\")\n",
    "        #print(f\"Masks shape: {masks.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        #print(f\"outputs shape: {outputs.shape}\")\n",
    "        #print(f\"masks shape: {outputs[0, :5, 0, 0]}\")\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            masks = masks.squeeze(1).long()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
    "\n",
    "print('Training complete')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
